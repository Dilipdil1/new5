# -*- coding: utf-8 -*-
"""Capstone_Project_YourCabs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rKgJjooJuebzxnwFm5KN-R60QT8_5zt
"""

# import libraries
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# connected with google drive
from google.colab import drive
drive.mount('/content/drive')

#  Load Data
df = pd.read_csv('/content/drive/MyDrive/Data science 2026/Data/YourCabs.csv')

# Display 3 rows
df.head(3)

# checking, How many rows and columns?.
df.shape

"""which columns drop? - why. which columns create new columns?. why."""

# id -drop- unique
# user_id -drop - Personal Information Identity
# vehicle_model_id - keep - without vehicle number_id, how passenger travel?. vehicle is important.
# package_id - drop - soo much columns value is missing.
# travel_type_id - keep - 'YourCab' company gives 3 types of services is important.
# from_area_id - drop - pincode or area_name is not usefull
# to_area_id - drop - pincode or area_name is not usefull
# from_city_id - drop - city name is not useful for predictions booking unavailable car cancel or not cancel.
# to_city_id -drop - city name is not useful for predictions boooking unavailable car cancel or not cancel.
# from_date - create -hours, week, month create columns.
# online_booking - keep - online booking find cancel or not cancel unavailablity car.
# mobile_site_booking - keep - mobile_site_booking find cancel or not cancel unavailablity car.
# booking_created - create - from_date & booking_created time difference.
# from_lat - keep -  journey starting place.
# from_long - keep - journey starting place.
# to_lat - drop - so much missing value.
# to_from -drop - so much missing value.
# Car_Cancellation - keep - Target.

# checking all dtype columns & null values.
df.info() # from_date & booking_created dtype is object.

# Change datatype object to  datatimeformate.
df['from_date'] = pd.to_datetime(df['from_date'], errors='coerce')
df['booking_created'] = pd.to_datetime(df['booking_created'], errors='coerce')

df.shape

df.from_date.isnull().sum() # Total rows=43,431. 'Nat' values present in columns=26,001. 50% above data missing, i cann't delete columns because from_date column is important.

df.booking_created.isnull().sum() # Total rows=43,431. 'Nat' values present in columns=26,007. 50% above data missing, i cann't delete columns because 'booking_created' column is important.

# Drop rows with invalid dates.
df = df.dropna(subset=['from_date', 'booking_created'])

df.shape

# Time difference (How many hours in advance was the booking made?
df['time_diff'] = (df['from_date'] - df['booking_created']).dt.total_seconds() / 3600

#  Extract date components
df['from_hour'] = df['from_date'].dt.hour
df['from_day_of_week'] = df['from_date'].dt.dayofweek
df['from_month'] = df['from_date'].dt.month

# first 5 row data display.
df.head(3)

df1=df # I create 'df1' dataframe because get important features.

plt.figure(figsize=(10, 6))
a=df.corr()
sns.heatmap(a,annot=True)
plt.show()

# drop unwanted columns.
df.drop(columns=['id','user_id','package_id','from_area_id','to_area_id','from_city_id','to_city_id','to_lat','to_long','from_date','booking_created'], inplace=True)

df.head(3)

print((df.isnull().sum().sum()/ (df.shape[0]*df.shape[1]))*100) # overall dataframe null value present 0.035% out of 10%. use fill value
(df.isnull().sum()/ df.shape[0])* 100                                  # 5% above remove columns.

# from_lat & from_long filling value with mode because it area id.
df.fillna({"from_lat":df["from_lat"].mode()[0],"from_long":df["from_long"].mode()[0]}, inplace=True)

# checking all null values is remove or not.
df.isnull().sum()

df.shape

# creating x is features, y is target.
x=df.drop(columns=['Car_Cancellation'])
y=df['Car_Cancellation']

x.shape

x.head(2)

y.head(2)

# imbalanced data sets.
y.value_counts()

# pip install imblearn
from imblearn.over_sampling import RandomOverSampler

RO=RandomOverSampler()
x, y =RO.fit_resample(x,y)
df2=pd.DataFrame(x)
df2['target']=y
df2['target'].value_counts()

# Train-Test Split
# 70% of data for training, 30% for testing
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Model Training (Random Forest)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluation
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")   #  '0' is booking not cancel, '1' is booking cancel.
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)





features=df1.columns
print(features)

# Feature Importance Visualization
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(x.shape[1]), importances[indices], align="center")
plt.xticks(range(x.shape[1]), [features[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()

